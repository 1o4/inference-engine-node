# API Documentation
## Introduction
This is the API doc of inference-engine-node.

## Interfaces
### InferenceEngine
```webidl
dictonary ApiVersion {
  long major;
  long minor;
};

dictionary Version {
  DOMString description;
  DOMString buildNumber;
  ApiVersion apiVersion;
};

interface InferenceEngine {
    Version getVersion();
    Core createCore();
};
```
### Core
```webidl
interface PluginVersions {
  readonly maplike<DOMString, Version>;
};

interface Core {
  PluginVersions getVersions(DOMString deviceName);
  Promise<Network> readNetwork(DOMString modelFilePath, DOMString weightsFilePath);
  Promise<ExecutableNetwork> loadNetwork(Network network, DOMString deviceName);
};
```
### Network
```webidl
enum Precision {
  'unspecified',
  'mixed',
  'fp32',
  'fp16',
  'q87',
  'i16',
  'u8',
  'i8',
  'u16',
  'i32',
  'i64',
  'bin',
  'custom'
};

enum Layout {
  'any',
  'nchw',
  'nhwc',
  'ncdhw',
  'ndhwc',
  'oihw',
  'scalar',
  'c',
  'chw',
  'hw',
  'nc',
  'cn',
  'blocked'
};

interface InputInfo {
  DOMString name();
  Precision getPrecision();
  void setPrecision(Precision precision);
  Layout getLayout();
  void setLayout(Layout layout);
  sequence<unsigned long long> getDims();
};

interface OutputInfo {
  DOMString name();
  Precision getPrecision();
  void setPrecision(Precision precision);
  Layout getLayout();
  sequence<unsigned long long> getDims();
};

interface Network {
  DOMString getName();
  sequence<InputInfo> getInputsInfo();
  sequence<OutputInfo> getOutputsInfo();
};
```
### ExecutableNetwork
```webidl
interface Blob {
  ArrayBuffer buffer();
  unsigned long byteSize();
  unsigned long size();
};

interface InferRequest {
  Blob getBlob(DOMString name);
  void infer();
  Promise<void> startAsync();
};

interface ExecutableNetwork {
  InferRequest createInferRequest();  
};
```
## Example
```js
// ----------- 1. Load inference engine instance -------------------------------
const core = ie.createCore();
// -----------------------------------------------------------------------------

// ----------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ----
const network = await core.readNetwork(model_xml, model_bin);
// -----------------------------------------------------------------------------

// ----------- 3. Configure input & output -------------------------------------
// -------------- Prepare input blobs ------------------------------------------
const input_info = network.getInputsInfo()[0];
input_info.setLayout('nhwc');
input_info.setPrecision('u8');

// -------------- Prepare output blobs -----------------------------------------
const output_info = network.getOutputsInfo()[0];
output_info.setPrecision('fp32');
// -----------------------------------------------------------------------------

// ----------- 4. Loading model to the device ----------------------------------
const executable_network = await core.loadNetwork(network, 'CPU');
// -----------------------------------------------------------------------------

// ----------- 5. Create infer request -----------------------------------------
const infer_request = executable_network.createInferRequest();
// -----------------------------------------------------------------------------

// ----------- 6. Prepare input ------------------------------------------------
const input_blob = infer_request.getBlob(input_info.name());
const input_data = new Uint8Array(input_blob.buffer());
// fill input_data
// -----------------------------------------------------------------------------

// ----------- 7. Do inference asynchronously-----------------------------------
await infer_request.startAsync();
// -----------------------------------------------------------------------------

// ----------- 8. Process output -----------------------------------------------
const output_blob = infer_request.getBlob(output_info.name());
const output_data = new Float32Array(output_blob.buffer());
// process output_data
// -----------------------------------------------------------------------------
```
